\section{Threats to Validity}
\label{threat}

%Conclusion validity is connected with how well it is possible to establish relationships between treatments and outcomes. Threats to conclusion validity often come from inappropriate use of statistics. 

\textbf{Conclusion validity.} 
%Threats to conclusion validity often come from inappropriate use of statistics. 
In the context of our survey study, we apply different non-parametric statistical tests appropriate for the cases where data was categorical (correctness, Chi-square test of independence) and continuous (time, Mann-Whitney U test). Furthermore, besides reporting p-values, we also report effect size measures appropriate for each scenario (odds-ratio and Cliff's delta) and apply a p-value correction technique to avoid the multiple testing problem. Finally, it could be argued that the size of the samples is insufficient to make conclusions for some of the atoms, a common problem in Empirical Software Engineering. We estimate the sample size for each one of the atom candidates, considering that each one had a different number of samples (Table~\ref{tab:difference-correctness}). To that end, we use the $\phi$ measure of effect size for each atom (based on the Chi-square statistic), the standard $\alpha$ coefficient of 0.05, and set the expected statistical power to 0.8, as usually employed in the literature~\cite{Ellis:2010:EGE}. We find out that the sample size is sufficient for all but one of the atoms where we had a statistically significant result for correctness, Implicit Predicate. 
% For the ones where we did not find statistically significant results for correctness, in fact the sample sizes
% we have analyzed are insufficient for such small effect sizes. This indicates that further studies on these
% atoms are required, due to the likelihood of type II errors.

%Construct validity is connected with how well the selected measures actually represent the concept of interest. 

\textbf{Construct validity.} We use correctness as a proxy for program comprehension and predicted outcomes of small programs as a measure of correctness. As discussed elsewhere~\cite{Oliveira:2020:ECR}, this approach is a test of the developers' ability to trace programs. Although this is a common approach in studies about atoms of confusion~\cite{TheEyesDoNotLie,Langhout:2021:ACJ,DBLP:conf/sigsoft/GopsteinIYDZYC17}, other measures of correctness could have been employed, potentially yielding different results. As a complement to correctness in the survey, we have measured the time it took for the participants to correctly predict the outcome of the code snippets (either with or without atom candidates). Furthermore, we have  asked the interviewees about their preferences when comparing confusing and clean versions of the programs.

%A potential problem with our method is that there little incentive for subjects to think thoroughly about the questions. We observed a lack of engagement when we ran the survey with undergraduate students during a pilot study. Although our final subjects were voluntarily partaking in the survey, we could not be sure that, after some time taking the survey, respondents would become tired and stop thinking clearly about the code.


%Internal validity is concerned with how well the study isolates the variables of interest and accounts for confounding factors. 

\textbf{Internal validity.} Since the survey was conducted online with unknown participants, we have no way of confirming their levels of education and experience. We mitigate this threat by, in the data analysis, explicitly accounting for programmer experience and using an experimental design that allows us to isolate the impact of the treatment from factors such as experience and formal education level. Also, we did not have a way to prevent respondents from cheating, such as running the code on an interpreter, or consulting other people. We partially mitigate this threat by presenting images with source code, instead of text. This creates an obstacle for participants to run the code while taking the survey.

%External validity is linked to whether it is possible to extrapolate the results of the study. 

\textbf{External validity.} Our results suggest that the selected idioms and code constructs may lead to confusion for small code snippets, but it is not clear if that result extrapolates to other scenarios. Even though it is likely that in larger code bases the confusion induced by these constructs may be even greater, the existence of additional context may mitigate this effect. Another possible threat to the generalizability of our results, in particular for the survey and interviews, lies in the fact that the analyzed atoms may rarely occur in real JavaScript code bases. To mitigate that threat, we have analyzed 72 popular open source JavaScript repositories and found out that most of them are common, occurring at least once per 1,000 lines of code. Only Arithmetic as Logic and Comma Operator occur less frequently than once per 10,000 lines of code. 
% \castor{A forma como os átomos foram detectados é uma ameaça que não discutimos.}


%% Finally, in one of the atoms, namely the Omitted Curly Braces,
%% we intentionally removed indentation from the original code,
%% which is highly unusual, given that many programmers use automatic
%% formatting in their code editors. This can introduce some level
%% of artificiality to this atom's question. Nonetheless,
%% we discovered that JavaScript does allow the programmer
%% to omit the curly braces after \textit{if} statements,
%% and insert multiple statements in the following line.
%% This fact itself might constitute
%% a source of confusion, which we leave to analyse
%% in our future endeavors. 
