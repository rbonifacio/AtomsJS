\section{Study Settings}
\label{method}

The main goal of this research is to investigate the impact of atoms of confusion in JavaScript code comprehension. As such, in this paper we answer the following research questions: 

\begin{enumerate}[(RQ1)]
    \item Do JavaScript developers identify atoms of confusion as contributing to program misunderstanding? 
    \item What is the impact of atoms of confusion on the comprehension of JavaScript code? 
    \item What are the particular JavaScript idioms and language constructs that might cause source code misunderstanding?
    \item What is the frequency of occurrence of atoms of confusion in practice (i.e., in open-source JavaScript projects)?
\end{enumerate}

 
Answering these research questions has several implications. For instance, we can either generalize or refute the perceptions about atoms of confusion already discussed in the literature (goal of research questions (RQ1) and (RQ2)). In addition, answers to the third and fourth questions allow us to enrich existing catalogs about atoms of confusion and discuss how often they occur in practice. Answering these research questions also lay the foundations for the implementation of libraries that can automatically transform code into its cleaner versions---though we postpone these results to a future research work. To answer these research questions we conduct a mixed-methods study, including {\color{red}two surveys}, a set of interviews, and an effort of mining JavaScript software repositories.  


\subsection{First Study: Survey-I}



In the firt study we explore the impact of atoms of confusion on
understanding JavaScript code, by answering two questions: (a) \emph{Do code snippets that contain atoms of confusion produce a higher misunderstanding rate when programmers try to predict their outcome?} and (b) \emph{Do code snippets with atoms of confusion require programmers to take longer to predict their output?}
Since JavaScript and C have some constructs in common (Section~\ref{sec:aoc}), we first selected a set of atoms of confusion for the \clang language~\cite{DBLP:conf/sigsoft/GopsteinIYDZYC17} whose idioms also exist in JavaScript programs. Appendix~\ref{sec:appendix-atoms} summarizes all atoms we consider in our research. 

\subsubsection{Survey design} 

The design of our first study blocks two variables
(developer experience and the code snippets) and
considers two treatments: the presence or absence of 
atoms of confusion within the code snippets. 
To achieve such a design goal, controlling the effect of experience and individual code snippets, we resorted to the \textit{Latin Square Design} \cite{Hunter-Experimenters}. Using this design we create a 2 x 2 matrix in which each row represents a subject and each column indicates the set of code snippets. The design of each square (a replica) is such that no treatment is repeated in the same row or column. For example, considering that we have a set of 10
code samples $s_0, s_1, ..., s_{10}$, if a given subject (P1) is asked to predict the output of the code samples $s_0, s_1, ..., s_5$ that contain atom candidates, then, when answering questions about non-confusing code snippets, they will only be presented with non-confusing versions of the code samples $s_6, s_7,..., s_{10}$. Furthermore, a given subject (P2), which constitutes the second row of our example square, will be asked questions about the non-confusing versions for $s_1, s_2, ..., s_5$, and will answer questions about confusing snippets of for $s_6, s_7,..., s_{10}$. By doing that, we guarantee that all versions of code snippets are contained within each square, and that each configuration occurs only once within a square. Figure \ref{fig:latinsquare} offers a visual representation of the concept.

  \begin{figure}[htb!]
      \noindent
      \centering
      \includegraphics[scale=.50]{images/latin-square.pdf}
      \caption{Latin square design. Each ``square'' corresponds to 
      a replica in our study. Each replica comprises two students (square rows) 
      and two sets of code snippets (CS-1 and CS-2). We randomly apply the 
      treatments (atom or non-atom code) to the cells of the squares.} 
      \label{fig:latinsquare}
  \end{figure}


Having selected 10 atoms in our first study, we wrote small pieces of code that contained each atom. We also wrote their equivalent snippet without the confusing idioms and constructs, leading to a total of 20 code snippets. In order to reduce the cognitive effort, we decided that each subject would be asked to predict the output of a subset of 10 listings, wherein each subset contained 5 blocks that contained atoms of confusion, whilst the remaining 5 had the atoms removed. The order in which the questions were presented was randomized. By doing this, we were seeking to minimize the chances of subjects being aware that the current listing they were analyzing contained (or not) atoms of confusion.
That is, each respondent of the survey should indicate what would be the outcomes of the code snippets, some of them having atoms of confusion (while other code snippets did not). 
We measured answer correctness and the total time each participant needed to answer reach question of the survey. 

\subsubsection{Survey Instrument} 

We implemented our survey as a web application. As part of this 
effort, we carried out an informal pilot whose main objectives were: to spot bugs in the application and in the data collection mechanism; to gain feedback from respondents about the user experience of the application; and to formulate an estimate about how long answering the survey would take on average. Fellow undergraduate students, professional colleagues, and friends took the pilot survey. Some users reported layout defects, and many reported that the landing page did not explain the survey well enough. We also spotted minor issues with our routines to create and populate the Latin Squares. 

We organized the survey in {\color{red}three} sections. The first section aims to characterize the subjects, asking for their age, education level, and programming experience. We also included a check button, whose checking meant users agreed that all collected data would be used solely for research purposes. In the second section, we presented to the participants a small set of instructions, where we explained how the survey worked and asked them to dedicate their attention to it. We stressed to participants the importance of not using any aids during the survey, such as online or console interpreters. For each question page, we kept track of whether or not the subjects switched windows. 

The next section of the survey presented a sequence of 10 questions, each containing a code snippet. For each question, there was a text box where the answer should be written. There was also an ``I do not know'' button, which, when clicked, led the subject to the next question. In our setting, ``I do not know'' was treated as a wrong answer. The code snippets were presented as images copied from a text editor, so as to demotivate respondents from resorting to external resources by copying and pasting the code into an interpreter. Upon submitting their answer for a particular question, the subject was automatically led to a similar page, containing another snippet.
    
We decided not to provide feedback about the time students took to answer each question. Nor did we tell them whether their answers were correct or not. Our main concern was to avoid introducing bias for future respondents. Since we posted the survey in a social media platform, if we gave respondents instant feedback, they might post comments on particular atoms, therefore interfering with future participants' thought process.

% \rb{nao acho esse paragrafo necessario} 
% \adriano{o de baixo ne? eu tambem acho}

% As we mentioned before, we first wrote the code listings in a text editor, and took pictures of it. In the case of an atom of confusion that was exclusive to JavaScript, which we called \textit{Automatic Semicolon Insertion} (see Appendix~\ref{sec:appendix-atoms}), it was necessary to remove the syntax highlighter. Even though semicolons at the end of statements are optional to programmers in JavaScript, the interpreter automatically inserts them into the code. Our text editor was incorrectly highlighting a line break after a return statement, even thought it was valid JavaScript syntax. We had thus to turn the highlighter off to take the picture of this atom. 

\subsubsection{Survey audience}

%% \diego{Esses detalhes são possíveis candidatos para remoção, se precisarmos de espaço}
%% {\color{blue}We invited developers to answer our survey by sending invitations to communities of JavaScript programmers on the Internet. Our initial plan was to try to engage contributors for two major JavaScript projects, namely Node.js and NPM. None of the projects, though, offered a direct way to interact with the community, such as an online discussion forums or mailing list. We would be required to first make contact with the projects' leaders, and only then would we have a chance to approach potential respondents. Due to time constraints, we decided to look for respondents elsewhere.}
We posted the survey on Reddit.\footnote{Reddit is a North American online discussion platform. Its discussion threads, often called subreddits, are sorted by subject. For our research, we posted information and links to the survey in two JavaScript subreddits.}. We explained our research purposes, and asked developers of any level of expertise to take the survey. To incentivize serious engagement, we proposed to raffle \$50 gift cards on Amazon products at the end of the survey. Within twelve hours we collected more than 150 answers, populating more than 70 replicas of the Latin Squares. We were able to collect significant data on time taken and discrepancies in answer correctness between confusing and non-confusing versions of the snippets. Some inconsistencies arose while building the squares, for instannce, when a user quit in the middle of the survey. When this happened, the corresponding row in the square was left incomplete. During data analysis, we considered all squares which contained incomplete rows to be invalid, and discarded them. Since we had a large enough number of samples, the squares we had to discard did not impact our analysis.
    
 
\subsection{Third Study: Interviews with Practitioners}

To complement our initial survey, we performed semi-structured interviews with professional JavaScript developers, aiming to identify their perceptions regarding code snippets containing atoms of confusion. We also asked each participant if they knew of any other JavaScript-specific construct that they regarded as confusing. In this section we details the protocol we followed to conduct the interviews and analyze the results.

% Nós realizamos entrevistas semi-estruturas com o objetivo de identificar a percepção dos desenvolvedores com experiência em JavaScript sobre algumas questões relacionadas à compreensão de código em JavaScript. Assim, nesta Seção nós descrevemos os procedimentos adotados para selecionar os participantes para as entrevistas e detalhamos como as entrevistas foram conduzidas. Além disso, detalhamos como os resultados das entrevistas foram analisados.

\subsubsection{Participants Selection} We invited the participants of the interviews using a snow balling technique. That is, starting from our network of contacts, we invited an initial set of candidates to take part of our survey. From this initial list, we asked for indication of additional candidates. Our main criterion of selection was that all participants shoukd have to been working professionally with JavaScript. We invited a total of 17 developers, and 15 of these agreed to participate. We conducted the interviews during a period of two weeks.


\subsubsection{Interview Process} We conducted semi-structured interviews using a web conferencing software. All the interviews were recorded with the consent of the participants. On average, the interviews lasted 26.29 minutes, with the shortest one lasting 14.59 minutes, and the longest one 43.06 minutes. The main goal of the interviews was to have developers assess in real time whether two versions of code (one with a single atom and another one without the atom) with the same behavior differed in readability, and, if so, which version they regarded as the easier one to understand. The interviews were conducted by two of the authors of this paper, and a third one listened to all the recordings to cross-validate the collected data.


The interviews had three main parts. In the first one, we asked the developers the following demographic information: name, email, gender, level of education, current job position, JavaScript experience in years and other programming languages they have worked with.
Table \ref{pinterview} summarizes this demographic information.

\begin{table*}[htb!]
\centering
\begin{tabular}
{p{0.4cm}p{0.9cm}p{2.0cm}p{6cm}p{1.9cm} p{4.5cm}}
\toprule
ID & Gender & Level & Role & JS Experience & Other Languages \\ \midrule 
P1 & Male & Graduate & Software Developer & 9 years & Java, PHP, C, Go  
\\ 
P2 & Male & High School & Software Developer & 3 years & Python, Go, Dart, Lua, C++, C\#
\\ 
P3 & Male & Graduate & Software Developer & 4 years & Java, C
\\ 
P4 & Male & Undergraduate & Software Developer & 3 years & Python, C, C++, Java, Go
\\ 
P5 & Male & Master Student & Software Developer & 3 years & Python, SQL
\\ 
P6 & Male & Graduate & Software Architect  & 15  years  & Java, PHP, C, Python , Ruby, C\# and R
\\ 
P7 & Male & Graduate & Software Developer  & 6 years & C, C++, Java, Assembly, Kotlin
\\ 
P8 & Male & PhD & Software Developer & 2 years & Java, Python
\\ 
P9 & Male & Graduate & Business Analyst and Software Developer & 5 years & PHP
\\ 
P10 & Male & Master Student & Business Analyst and Software Developer  & 4 years & Java, C\# and Python
\\ 
P11 & Male & Master Student & Software Architect & 4 years & Java, Erlang, C\#, Cobol
\\ 
P12 & Female & Master & Software Developer  & 1 year & C
\\ 
P13 & Male & Master & Software Developer & 13 years & Java, PHP
\\ 
P14 & Female & Master Student & Software Developer  & 3 years & C, Python, Ruby
\\ 
P15 & Female &  Graduate & Software Developer & 2 years & Java, PHP
\\ \bottomrule
\end{tabular}
    \caption{Demographic information of the participants}
    \label{pinterview}
\end{table*}

In the second part of the interview our aim was to allow the subjects to describe
their JavaScript experience, as well as to allow them to reveal any JavaScript's constructs they regarded as innately confusing. This would allow us to identify potential atom candidates that are more specific to the JavaScript language. The questions we explored in this section include:

\begin{itemize}
\item In general, do you implement new JavaScript components,
  or do you maintain components developed by other programmers?;

\item In your opinion, what are the main pros and cons of the JavaScript language?;

\item Do you think JavaScript is a language that produces code that is hard to understand?

\item Do you regard any particular construct or idiom of the language as
  especially confusing?
\end{itemize}


In the third part of the interview, participants were shown the 20 code snippets that were used in the first study. The participants were told both sides of each pair had the same output, and were asked to evaluate which version of the code was easier to understand. The interviewers restrained themselves from introducing bias into the answers by not explaining that one of the sides of each pair contained the atom under investigation. Subjects were just presented the snippets and allowed to take the necessary time to decide on the most readable snippet.

\subsubsection{Interview Analysis}

{\color{red}Nós realizamos a análise das entrevistas em pares. Inicialmente um dos participantes de todas as entrevistas transcreveu os resultados. Posteriormente, um dos autores do trabalho ouviu os áudios das entrevistas e transcreveu os resultados.} 

\textcolor{red}{Adriano teria algo mais para adicionar aqui na  análise das entrevistas?}

\todo[inline]{adriano:estou escrevendo uma tabela em que cada linha e um dos atomos, e colunas representam \% preferencia com atomo, \% preferencia sem atomo, \% indiferente. Mas acredito que esta tabela deve estar na section results, certo? O resto das informacoes deste paragrafo eu citei acima no Interview Process, entao acho que podemos abolir esta section. -- Edna: Concordo contigo Adriano}

\subsection{Third Study: Additional Survey and Interviews}

\rb{acho que aqui podemos incluir o segundo survey e a segunda
  rodada das entrevistas, explorando os \'{a}tomos adicionais}


\subsection{Fourth Study: Mining Software Repositories}

To understand how often atoms of confusion appear in real settings, and thus answer our fourth research question (\emph{What is the frequency of occurrence of atoms of confusion in practice?}), we mined a set of GitHub open source repositories. To this end, we first collected the most popular GitHub repositories that are primarily written in JavaScript. We measured popularity using the project's stargazers. This metric, available on  GitHub API, represents the number of stars a project received from users of the platform. The same metric has been used in a number of previous studies as a proxy to estimate project's popularity~\cite{gyimesi2019bugsjs,canedo:esem2020}. We then selected the top 100 most popular repositories and removed projects that did not reach the first quartile of the distribution of lines of code.

After filtering out JavaScript project candidates, in the second step we built a curate dataset comprising the top \minedprojects repositories. Examples of projects in this dataset include \textsc{React}, \textsc{Node JS}, and \textsc{AngularJS}. Table~\ref{tab:projects-statistics} presents some statistics about the projects we consider in our research. The size of the projects range from small ones (5543 lines of code) to complex systems with more than 1 MLOC. All projects in our dataset have at least \num{1244} forks and \num{23672} stars. We automated all the steps to filter, clone, and collect the statistics from the repositories using Python scripts.

In the third step we mined atoms of confusion from the repositories in our curate dataset, using source code queries that we had written using the CodeQL language~\cite{moor:gttse2007}. CodeQL is an object-oriented variant of the Datalog language~\cite{rodriguez2020efficient}, and currently supports researchers and practitioners to query the source code of systems written in different languages (such as Java and JavaScript). We also automate the process o
f running the queries and exporting the results to a format that simplifies our
analysis (and also the reproduction of this study). In the final step we carry
out basic descriptive statistics, in order to understand how often the
atoms of confusion appear in practice. 

\begin{table*}[ht]
 \centering
 \begin{tabular}{rrrrrrr}
   \hline
             & Min. & 1st Qu. & Median & Mean & 3rd Qu. & Max. \\ \hline
 Lines of Code           & \num{5543}  & \num{16060.25} & \num{36161.5} & \num{111432.33} & \num{111182.50} & \num{1278405} \\
 Num. of Forks     & \num{1244}   & \num{3029.75} & \num{6078} & \num{8906.24} & \num{9260.50} & \num{68849} \\
 Num. of Contributors  & \num{6}   & \num{184} & \num{285.5} & \num{533.44} & \num{515.75} & \num{4047} \\
 Num. of Stars        & \num{23672} & \num{27055} & \num{34990} & \num{46919.51} & \num{48518} & \num{310935} \\
 
    \hline
 \end{tabular}
 \caption{Some descriptive statistics about the projects used in the study}
 \label{tab:projects-statistics} 
 \end{table*}


