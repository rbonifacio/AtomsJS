\section{Methodology}
\label{method}

The main goal of this research is to investigate the impact of atoms of confusion on JavaScript code comprehension. As such, in this paper we answer the following research questions: 

\begin{enumerate}[(RQ1)]
\item \rqa 
\item \rqb
% \item \rqc  
\item \rqd\castor{This one will probably be removed.}
\end{enumerate}


By answering these research questions, we can either generalize or refute the findings about atoms of confusion already discussed in the literature (goal of research questions (RQ1) and (RQ2)). In addition, answer to the third question allows us to enrich existing catalogs about atoms of confusion and discuss how often they occur in practice\castor{This should probably be removed.}. Answering these research questions also lays the foundations for the implementation of tools that can automatically transform code into cleaner versions.
%---though we postpone these results to future research work.
We conduct a mixed-methods study to answer these questions. It includes two independently designed and conducted experiments\castor{I would rather use the term ``experiment''.}: a replication of the first study on atoms of confusion, but targeting JavaScript programs, and a new study using a latin-square counterbalancing experimental design to control for subject experience. 
These two experiments explore the impact of atom candidates on understanding JavaScript code (goal of research question RQ1). They investigate whether or not code snippets that contain atom candidates produce a higher misunderstanding rate when programmers try to predict their outcome.\castor{I've removed the part about time because, in order for the first study to be a replication, we need to compute time differently, accounting for incorrect responses as well.}% and (b) require more time for programmers to predict their output. 
We leverage two independently-developed studies with different designs to more rigorously verify the impact of atoms of confusion in code comprehension activities. 
We also conduct a set of interviews with developers to contrast the quantitative results of the experiments with their preferences and opinions (thus addressing RQ2). 

\subsection{Replication Study Setting}\label{sec:meth:replication}

We call the first study we performed the \textit{``Replication Study''} because it is a partial replication~\cite{Juristo2012} of the first study on atoms of confusion, conducted by Gopstein et. al~\cite{DBLP:conf/sigsoft/GopsteinIYDZYC17}, but targeting JavaScript instead of the C language. Gopstein et. al. employed a randomized, partial counterbalancing experimental design on a sample of 73 subjects, mostly students, with three or more months of experience in C or C++. The control variable consisted of a tiny program containing a single atom candidate and the treatment a functionally-equivalent version that does not contain the atom. The main dependent variable is whether subjects are able to correctly determine the output of the programs. The other dependent variable is the time required to correctly vs. incorrectly determine the out of the programs. The study had the null hypothesis that \textit{``code from both control and treatment groups can be hand-evaluated with equal accuracy''} and the alternate hypothesis that \textit{``the existence of an atom of confusion causes more errors than other code in hand-evaluated outputs.''}. For each atom candidate, six programs were built, three versions including the atom candidate and three versions not including it. Each subject was exposed to four programs for every atom candidate, two including an atom and two not including an atom.  

\subsubsection*{Experimental design} 

In this study we adopt the same experimental design, control and treatment variables, dependent variable, and hypotheses. We have also built six programs for each atom candidate, three with and three without the atom. Considering the elements that can vary in a replication study, defined by Juristo and Gomez~\cite{Juristo2012}, in this study we employ the same experimental protocol and construct operationalizations. We have different population properties, experimenters, and a different site. In addition, since we focus on a different language, some atoms that exist in C cannot by construction exist in JavaScript programs. Conversely, JavaScript exhibits atom candidates that do not exist in C. 

The design of the replication study considers two treatments: the presence or absence of atoms candidates within the programs. To identify atom candidates, we start out by selecting every atom candidate from Gopstein's study that can also happen in JavaScript. That excludes atoms related to pointers and preprocessor directives. Furthermore, we conduct an informal search of programming forums, solutions to Code Golf~\footnote{https://codegolf.stackexchange.com/} problems, code style guidelines, and expert knowledge to identify new atom candidates. Overall, the replication study analyzes 24 atoms, 15 from the original study and 9 that are new and specific to JavaScript. Table~\ref{tab:atoms_replication} presents the complete list. 

For each atom candidate, we write six short programs, three including and three not including the atom candidate. The former are the control for the study or the obfuscated versions of the programs. The alternative, atom-free, versions of the programs are the treatment, or clean, versions. Each subject is exposed to one obfuscated and one clean version of each atom, totalizing 48 programs, and should determine their outputs. As mentioned before, we measure answer correctness and the time to answer questions correctly vs. incorrectly. We control for learning effects~\cite{Neely:1991:SPE} in three manners. First, by having multiple obfuscated and clean versions for each atom and presenting only one of each per subject. Second, by presenting the programs in a random order. Third, by presenting obfuscated and clean versions corresponding to the same atom candidate with at least 11 other programs in between, in  accordance to the original experimental protocol~\cite{DBLP:conf/sigsoft/GopsteinIYDZYC17}. 

\subsubsection*{Study instrument} 

The study is conducted by means of a web application. As part of this effort, we carried out an informal pilot whose main objectives were (i) to spot bugs in the application and in the data collection mechanism; (ii) to gain feedback from respondents about the user experience of the application; and (iii) to formulate an estimate about how long answering the survey would take on average. Based on the feedback of the participants of the pilot, we decided to present only one obfuscated and one clean version of each atom candidate, to reduce to total time of the study and potentially increase participation. Initially, our plan was to present two of each. 

We organize the study in three sections. In the first one we present instructions and also a check button whose checking meant users agreed that all collected data would be used anonymously and solely for research purposes. The instructions explain how the survey works and asks them to dedicate their attention to it.  We stressed to participants the importance of not using any aids during the survey, such as online or console interpreters. 

The second section presents the programs, one at a time. For each one, there is a text box where the answer should be written. There is also an ``I do not know'' button, which, when clicked, leads the subject to the next question. In our setting, ``I do not know'' was treated as a wrong answer. To reduce the likelihood of a subject leveraging attempting to execute the programs, the web application verifies whether the subject attempts to change tabs or windows and presents the programs as images, instead of text. Upon submitting an answer for a particular program, the subject is automatically led to a similar page, containing the next one.

We do not provide feedback about the time subjects take to answer each question. We also do not tell them whether their answers are correct or not. This aims at avoiding introducing bias for future respondents. Since we posted the survey on social media platforms, possible threats could have arisen if we gave respondents instant feedback.


\subsubsection*{Study audience}

We posted the survey on the authors' social media platforms, on Brazilian CS departments' mailing lists, and on two programming subreddits. We explained our research purposes and asked developers to take the survey only if they have some familiarity with JavaScript. We collected 70 responses. The average survey respondent has a mean 26 months of experience in JavaScript, with a median of 12. 


\subsection{Latin-Square Study Setting}\label{sec:meth:survey}

This experiment complements the replication study by using a different experimental design and controlling for one additional variable, the experience of the subjects. For this study we employ a subset of the atom candidates used in the replication study. More specifically, we
%Since JavaScript and \clang have some constructs in common (Section~\ref{back}), we first 
select a set of 9+1 atom candidates: nine previously-validated atoms of confusion for the \clang language~\cite{DBLP:conf/sigsoft/GopsteinIYDZYC17} that also exist in JavaScript programs plus one atom candidate that is specific to the JavaScript language (Automatic Semicolon Insertion)\castor{I think the name of this atom is inconsistent between the studies}. 
Our supplementary material discusses and shows examples of the atoms we consider in our research.\castor{We should probably do this here or just refer to the table that I intend to insert for the replication study.}

\subsubsection*{Experimental design} 

The design of our second study blocks two variables
(developer experience and the code snippets) and
considers two treatments: the presence or absence of 
atoms candidates within the code snippets. 
To achieve such a design goal, controlling the effect of experience and individual code snippets, we resorted to the \textit{Latin Square Design} \cite{Hunter-Experimenters}. Using this design we create a 2 x 2 matrix in which each row represents a subject and each column indicates the set of code snippets. The design of each square (a replica) is such that no treatment is repeated in the same row or column. For example, considering that we have a set of 10
code samples $s_0, s_1, ..., s_{10}$, if a given subject (P1) is asked to predict the output of the code samples $s_0, s_1, ..., s_5$ that contain atom candidates, then, when answering questions about non-confusing code snippets, they will only be presented with non-confusing versions of the code samples $s_6, s_7,..., s_{10}$. Furthermore, a given subject (P2), who constitutes the second row of our example square, will be asked questions about the non-confusing versions for $s_1, s_2, ..., s_5$, and will answer questions about confusing snippets for $s_6, s_7,..., s_{10}$. By doing that, we guarantee that all versions of the code snippets are contained within each square, and that each configuration occurs only once within a square. Figure \ref{fig:latinsquare} offers a visual representation of the concept.

  \begin{figure}[htb!]
      \noindent
      \centering
      \includegraphics[scale=.50]{images/latin-square.pdf}
      \caption{Latin square design. Each ``square'' corresponds to 
      a replica in our study. Each replica comprises two participants (square rows, e.g., P1 and P2) 
      and two sets of code snippets (CS-1 and CS-2). We randomly apply the 
      treatments (atom or non-atom code) to the cells of the squares.} 
      \label{fig:latinsquare}
  \end{figure}


Having selected 10 atom candidates in our first study, we wrote 10 short programs, one containing each atom candidate. We call them the confusing versions of the programs. We also wrote corresponding short programs without the confusing idioms and constructs and we call them the clean versions of the programs. Overall, the survey employed 20 code snippets, 10 confusing and 10 clean. In order to reduce the cognitive effort, each subject was asked to predict the output of 10 code snippets, five confusing and five clean. The order in which the questions were presented was randomized. By doing this, we seek to minimize the chances of subjects being aware that the current listing they are analyzing contains (or not) atoms of confusion. That is, each respondent of the survey should indicate what would be the outcomes of the code snippets, some of them having atoms of confusion (while other code snippets do not).  We measured answer correctness and the total time each participant needed to answer the survey's questions.

\subsubsection*{Study instrument} 

We implemented our survey as a web application and carried out a pilot, similarly to the replication study. 
%As part of this effort, we carried out an informal pilot whose main objectives were (i) to spot bugs in the application and in the data collection mechanism; (ii) to gain feedback from respondents about the user experience of the application; and (iii) to formulate an estimate about how long answering the survey would take on average.
Undergraduate students and professional colleagues took the pilot survey. Some users reported layout defects, and many reported that the landing page did not explain the survey well enough. We also spotted minor issues with our routines to create and populate the Latin Squares. 

We organized the survey in three sections. The first section aims to characterize the subjects, asking their age, education level, and programming experience. We also included a check button, whose checking meant users agreed that all collected data would be used solely for research purposes. In the second section, we presented to the participants a small set of instructions, where we explained how the survey worked and asked them to dedicate their attention to it. We stressed to participants the importance of not using any aids during the survey, such as online or console interpreters. For each question page, we kept track of whether or not the subjects switched windows. 

The last section of the survey presented a sequence of ten questions, each containing a code snippet. For each question, there was a text box where the answer should be written. There was also an ``I do not know'' button, which, when clicked, led the subject to the next question. In our setting, ``I do not know'' was treated as a wrong answer. The code snippets were presented as images copied from a text editor, so as to demotivate respondents from resorting to external resources by copying and pasting the code into an interpreter. Upon submitting their answer for a particular question, the subject was automatically led to a similar page, containing another snippet. Similarly to the replication study, we do not provide feedback about time and correctness to the subjects.


% \rb{nao acho esse paragrafo necessario} 
% \adriano{o de baixo ne? eu tambem acho}

% As we mentioned before, we first wrote the code listings in a text editor, and took pictures of it. In the case of an atom of confusion that was exclusive to JavaScript, which we called \textit{Automatic Semicolon Insertion} (see Appendix~\ref{sec:appendix-atoms}), it was necessary to remove the syntax highlighter. Even though semicolons at the end of statements are optional to programmers in JavaScript, the interpreter automatically inserts them into the code. Our text editor was incorrectly highlighting a line break after a return statement, even thought it was valid JavaScript syntax. We had thus to turn the highlighter off to take the picture of this atom. 

\subsubsection*{Study audience}

%% \diego{Esses detalhes são possíveis candidatos para remoção, se precisarmos de espaço}
%% {\color{blue}We invited developers to answer our survey by sending invitations to communities of JavaScript programmers on the Internet. Our initial plan was to try to engage contributors for two major JavaScript projects, namely Node.js and NPM. None of the projects, though, offered a direct way to interact with the community, such as an online discussion forums or mailing list. We would be required to first make contact with the projects' leaders, and only then would we have a chance to approach potential respondents. Due to time constraints, we decided to look for respondents elsewhere.}

We posted the survey on a JavaScript Reddit channel.\footnote{https://www.reddit.com/r/javascript/} We explained our research purposes and asked developers of any level of expertise to take the survey. Within twelve hours, we collected more than 150 answers, populating more than 70 replicas of the Latin Squares. We collected significant data on time taken and discrepancies in answer correctness between confusing and non-confusing versions of the snippets. Some inconsistencies arose while building the squares, for instance, when a user quit in the middle of the survey. We discarded from our study all squares that contained incomplete rows (a common approach for data imputation).
%Since we had a large enough number of samples, the squares we had to discard did not impact our analysis.

 
\subsection{Interview Study Setting}

To complement our initial survey, we performed semi-structured interviews with professional JavaScript developers, aiming to identify their perceptions regarding code snippets containing atoms of confusion (research question RQ2). We also asked each participant if they knew of any other JavaScript-specific construct or idiom that they regarded as being likely to make the code hard to understand (research question RQ3). In this section we detail the protocol we followed to conduct the interviews and to analyze the results.

% Nós realizamos entrevistas semi-estruturas com o objetivo de identificar a percepção dos desenvolvedores com experiência em JavaScript sobre algumas questões relacionadas à compreensão de código em JavaScript. Assim, nesta Seção nós descrevemos os procedimentos adotados para selecionar os participantes para as entrevistas e detalhamos como as entrevistas foram conduzidas. Além disso, detalhamos como os resultados das entrevistas foram analisados.

\subsubsection*{Participant Selection} We invited the participants of the interviews using a snowballing technique. That is, starting from our network of contacts, we invited an initial set of candidates to take part in our survey. From this initial list, we asked for an indication of additional candidates. Our main selection criterion was that all participants should have been working with JavaScript in their daily professional activities. We invited a total of 17 developers, and 15 of them agreed to participate.   
%We conducted the interviews for a period of two weeks.


\subsubsection*{Interview Process} We conducted semi-structured interviews using web conferencing software. We recorded all the interviews with the consent of the participants. On average, the interviews lasted 26.29 minutes, with the shortest one lasting 14.59 minutes and the longest one 43.06 minutes. 
Two of the researchers conducted the interviews, and a third one listened to all the recordings to cross-validate the collected data.
The interviews had three main parts. In the first one, we asked the developers the following demographic information: name, email, gender, level of education, current job position, JavaScript experience in years, and other programming languages they have worked with.
Table \ref{pinterview} summarizes this demographic information.

\begin{table}[htb!]
  \centering
  \caption{Demographic information of the participants}
\begin{scriptsize}  
\begin{tabular}{clcl}
\toprule
ID & Education & JS Experience & Other Languages \\ \midrule 
P1 & BSc Degree & 9 years & Java, PHP, C, Go  
\\ 
P2 & HS Degree & 3 years & Python, Go, Dart, Lua, C++, C\#
\\ 
P3 & BSc Degree & 4 years & Java, C
\\ 
P4 & Undergraduate & 3 years & Python, C, C++, Java, Go
\\ 
P5 & Master Student & 3 years & Python, SQL
\\ 
P6 & Bsc Degree & 15  years  & Java, PHP, C, Python , Ruby, C\#
\\ 
P7 & BSc Degree  & 6 years & C, C++, Java, Assembly, Kotlin
\\ 
P8 & PhD Degree & 2 years & Java, Python
\\ 
P9 & BSc Degree  & 5 years & PHP
\\ 
P10 & Master Student & 4 years & Java, C\# and Python
\\ 
P11 & Master Student & 4 years & Java, Erlang, C\#, Cobol
\\ 
P12 & Master Degree & 1 year & C
\\ 
P13 & Master Degree & 13 years & Java, PHP
\\ 
P14 & Master Student & 3 years & C, Python, Ruby
\\ 
P15 &  BSc degree  & 2 years & Java, PHP
\\ \bottomrule
\end{tabular}
\end{scriptsize}
    \label{pinterview}
\end{table}

In the second part of the interview our aim was to allow the subjects to describe their JavaScript experience, as well as to allow them to reveal any JavaScript constructs they regarded as innately confusing. This would allow us to identify potential atom candidates that are more specific to the JavaScript language. Examples of questions we explored in this section include: \emph{Does JavaScript favor developers to produce code that is hard to understand?} and \emph{Do you regard any particular construct or idiom of the language as especially confusing?}

In the third part of the interview, participants were shown the code snippets that were used in the first study. The snippets were presented in pairs, where each pair contained a clean and a confusing version. The participants 
%were told both sides of each pair had the same output, and 
were asked to evaluate which version of the code was easier to understand. To avoid introducing bias in the answers, the interviewers did not explain that one of the versions in each pair contained the atom under investigation. Subjects were just presented the pairs of snippets and allowed to take the necessary time to decide on the most readable snippet.

\subsubsection*{Interview Analysis}

We first transcribed each interview and then examined the broad distribution of the answers. Our goal was to build an initial understanding of the participants' perceptions with regards to the challenges to understand JavaScript code in general and JavaScript code with atoms of confusion in particular. We next followed with an open-coding procedure, highlighting the main themes and quoting the answers of the participants. We present these results in Section~\ref{sec:interview-results}. 
%Finally, we consolidate the feedback of the participants with respect to the confusion that our set of atom candidates might introduce in JavaScript code.

%\subsection{Third Study: Additional Survey and Interviews}

% \rb{acho que aqui podemos incluir o segundo survey e a segunda
%  rodada das entrevistas, explorando os \'{a}tomos adicionais}


\subsection{Mining JavaScript Software Repositories Setting}

To understand how often the analyzed atom candidates appear in open source projects, and thus answer our fourth research question (\emph{What is the frequency of occurrence of atoms of confusion in practice?}), we mined a set of GitHub open source repositories. To this end, we first collected the most popular GitHub repositories that are primarily written in JavaScript. We measured popularity using the project's stargazers. This metric, available through the GitHub API, represents the number of stars a project received from users of the platform. The same metric has been used in a number of previous studies as a proxy to estimate a project's popularity~\cite{gyimesi2019bugsjs,canedo:esem2020}. We then selected the top 100 most popular repositories and removed projects that did not reach the first quartile of the distribution of lines of code.

After filtering out JavaScript project candidates, in the second step we built a curated dataset comprising the top \minedprojects repositories. Examples of projects in this dataset include \textsc{React}, \textsc{Node JS}, and \textsc{AngularJS}. Table~\ref{tab:projects-statistics} presents some statistics about the projects we consider in our research. The size of the projects range from small ones (5543 lines of code) to complex systems with more than 1 MLOC. All projects in our dataset have at least \num{1244} forks and at least \num{23672} stars. We automated all the steps to filter, clone, and collect the statistics from the repositories using Python scripts.

We mined the occurrence of atom candidates from the repositories in our curated dataset using source code queries that we wrote using the CodeQL language~\cite{moor:gttse2007}. CodeQL is an object-oriented variant of the Datalog language~\cite{rodriguez2020efficient}, and currently supports researchers and practitioners to query the source code of systems written in different languages (such as \cpplang, Java, and JavaScript). We also automate the process of running the queries and exporting the results to a format that simplifies our analysis (and also the reproduction of this study). Finally, we computed some descriptive statistics to measure the prevalence of atoms of confusion in practice. 

\begin{table}[ht]
  \centering
   \caption{Some descriptive statistics about the projects used in the MSR study}
  \begin{scriptsize}
 \begin{tabular}{rrrrr}
   \hline
                       & Min.             & Median        & Mean             & Max. \\ \hline
 Lines of Code         & \num{5543}       & \num{36161.5} & \num{111432.33}  & \num{1278405} \\
 Num. of Forks         & \num{1244}       & \num{6078}    & \num{8906.24}    & \num{68849} \\
 Num. of Contributors  & \num{6}          & \num{285.5}   & \num{533.44}     & \num{4047} \\
 Num. of Stars         & \num{23672}      & \num{34990}   & \num{46919.51}   & \num{310935} \\
 
    \hline
 \end{tabular}
 \end{scriptsize}
 \label{tab:projects-statistics} 
 \end{table}


